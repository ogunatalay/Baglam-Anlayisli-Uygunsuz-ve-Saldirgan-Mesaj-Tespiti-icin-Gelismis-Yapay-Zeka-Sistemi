# -*- coding: utf-8 -*-
"""DDÄ° PROJESÄ°.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1egbARnEPFagC5QS_p_W2KfZ3MhaWomqB
"""



import pandas as pd

# DosyayÄ± oku
df = pd.read_csv("labeled_data.csv")

# Sadece tweet ve class sÃ¼tunlarÄ±nÄ± al
df = df[['tweet', 'class']]

# Ä°lk 5 satÄ±rÄ± gÃ¶relim
df.head()

import re
import nltk
import pandas as pd
from nltk.corpus import stopwords

# Ä°lk kez Ã§alÄ±ÅŸtÄ±rÄ±yorsan aÅŸaÄŸÄ±dakileri indir
nltk.download('stopwords')

# TÃ¼rkÃ§e ve Ä°ngilizce stopword'leri birleÅŸtir
stop_words = set(stopwords.words('english')).union(set(stopwords.words('turkish')))

def clean_text(text):
    text = text.lower()  # KÃ¼Ã§Ã¼k harfe Ã§evir
    text = re.sub(r"http\S+", "", text)  # Linkleri sil
    text = re.sub(r"@\w+", "", text)  # MentionlarÄ± sil
    text = re.sub(r"#\w+", "", text)  # Hashtagleri sil
    text = re.sub(r"[^\w\s]", "", text)  # Noktalama iÅŸaretlerini sil
    text = re.sub(r"\d+", "", text)  # SayÄ±larÄ± sil
    text = re.sub(r"\s+", " ", text).strip()  # Fazla boÅŸluklarÄ± temizle
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]  # Stopword'leri Ã§Ä±kar
    return " ".join(tokens)

df['CLEAN_TEXT'] = df['tweet'].apply(clean_text)

# Ä°lk 5 satÄ±rÄ± gÃ¶relim
df[['tweet', 'CLEAN_TEXT', 'class']].head()

df.to_csv("cleaned_labeled_data.csv", index=False)

from google.colab import files
files.download("cleaned_labeled_data.csv")

!pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# SÄ±nÄ±f adlarÄ±nÄ± daha anlamlÄ± hale getirelim
class_labels = {
    0: "Hate Speech",
    1: "Offensive Language",
    2: "Neither"
}

# WordCloud Ã§izimi iÃ§in fonksiyon
def plot_wordcloud(data, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(data))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontsize=18)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# Her sÄ±nÄ±f iÃ§in ayrÄ± ayrÄ± word cloud Ã§izelim
for class_id, label in class_labels.items():
    class_data = df[df['class'] == class_id]['CLEAN_TEXT']
    plot_wordcloud(class_data, f"WordCloud - {label}")

from sklearn.model_selection import train_test_split
# Ã–zellik ve etiketleri belirle
X = df['CLEAN_TEXT']  # Girdi olarak temiz metinler
y = df['class']       # Hedef sÄ±nÄ±flar (0, 1, 2)

# EÄŸitim ve test olarak ayÄ±r (Ã¶rnek: %80 eÄŸitim, %20 test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Boyutlara bakalÄ±m
print("EÄŸitim verisi:", len(X_train))
print("Test verisi  :", len(X_test))

# Her sÄ±nÄ±ftaki Ã¶rnek sayÄ±sÄ±
class_counts = df['class'].value_counts().sort_index()
print(class_counts)
# SÄ±nÄ±f adlarÄ±yla etiketleme
class_labels = {
    0: "Hate Speech",
    1: "Offensive Language",
    2: "Neither"
}

# SÄ±nÄ±f adlarÄ±nÄ± etiketleyerek gÃ¶ster
for cls_id, count in class_counts.items():
    print(f"{class_labels[cls_id]}: {count} tweet")
import matplotlib.pyplot as plt

# Grafik Ã§izimi
plt.figure(figsize=(8, 5))
plt.bar(class_counts.index.map(class_labels), class_counts.values, color=["#e74c3c", "#f1c40f", "#2ecc71"])
plt.title("Her SÄ±nÄ±f iÃ§in Veri SayÄ±sÄ±", fontsize=16)
plt.xlabel("SÄ±nÄ±f", fontsize=12)
plt.ylabel("Veri SayÄ±sÄ±", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Ã–rnek olarak tweet dizilimini baÄŸlamsal hale getiriyoruz
context_inputs = []
previous_texts = df['CLEAN_TEXT'].tolist()

for i in range(len(previous_texts)):
    if i == 0:
        context_inputs.append(previous_texts[i])
    else:
        combined = previous_texts[i-1] + " [SEP] " + previous_texts[i]
        context_inputs.append(combined)

# Yeni sÃ¼tun ekle
df['CONTEXT_INPUT'] = context_inputs

# Ä°lk 5 Ã¶rneÄŸi gÃ¶relim
df[['CONTEXT_INPUT', 'class']].head()

# context_input iÃ§eren veri setini kaydet
df.to_csv("context_aware_labeled_data.csv", index=False)

from google.colab import files
files.download("context_aware_labeled_data.csv")

!pip install --upgrade transformers

# âœ… Gerekli Kurulumlar
!pip install --upgrade transformers datasets --quiet

# âœ… GPU kontrolÃ¼
import torch
print("GPU kullanÄ±labilir mi?", torch.cuda.is_available())

# âœ… Gerekli kÃ¼tÃ¼phaneler
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset

# âœ… 1. Veri YÃ¼kleme
df = pd.read_csv("context_aware_labeled_data.csv")

# âœ… 2. Ã–zellik ve etiket
X = df['CONTEXT_INPUT'].tolist()
y = df['class'].tolist()

# âœ… 3. Train/Test split (%80 - %20)
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# âœ… 4. Train/Validation split (%90 - %10)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, stratify=y_train_full, random_state=42)

# âœ… 5. Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# âœ… 6. Dataset sÄ±nÄ±fÄ±
class TweetDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

# âœ… 7. Datasetâ€™leri oluÅŸtur
train_dataset = TweetDataset(X_train, y_train)
val_dataset   = TweetDataset(X_val, y_val)
test_dataset  = TweetDataset(X_test, y_test)

# âœ… 8. Modeli yÃ¼kle
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

# âœ… 9. EÄŸitim ayarlarÄ±
training_args = TrainingArguments(
    output_dir="./results",
    logging_strategy="epoch",  # `evaluation_strategy` yerine `logging_strategy` kullanabilirsiniz
    save_strategy="no",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none"
)


# âœ… 10. Trainer nesnesi
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# âœ… 11. EÄŸitimi baÅŸlat
trainer.train()

# âœ… 12. Train seti Ã¼zerinde tahmin yap
train_preds_output = trainer.predict(train_dataset)
train_preds = torch.argmax(torch.tensor(train_preds_output.predictions), axis=1)
train_labels = torch.tensor(train_preds_output.label_ids)

# âœ… 13. Test seti Ã¼zerinde tahmin yap
test_preds_output = trainer.predict(test_dataset)
test_preds = torch.argmax(torch.tensor(test_preds_output.predictions), axis=1)
test_labels = torch.tensor(test_preds_output.label_ids)

class_names = ["Hate Speech", "Offensive", "Neither"]

# âœ… 14. SÄ±nÄ±flandÄ±rma Raporu (Classification Report)
print("\nğŸ“Š TRAIN SÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(train_labels, train_preds, target_names=class_names))

print("\nğŸ“Š TEST SÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(test_labels, test_preds, target_names=class_names))

# âœ… 15. Confusion Matrix (Train ve Test Setleri)
# Train CM
cm_train = confusion_matrix(train_labels, train_preds)
disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=class_names)
fig, ax = plt.subplots(figsize=(6, 6))
disp_train.plot(cmap="Greens", ax=ax, values_format='d')
plt.title("Confusion Matrix â€“ TRAIN Seti")
plt.show()

# Test CM
cm_test = confusion_matrix(test_labels, test_preds)
disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=class_names)
fig, ax = plt.subplots(figsize=(6, 6))
disp_test.plot(cmap="Blues", ax=ax, values_format='d')
plt.title("Confusion Matrix â€“ TEST Seti")
plt.show()

# âœ… 16. Loss Grafiklerini Ã‡iz (Train ve Validation)
logs = trainer.state.log_history

train_loss = [log["loss"] for log in logs if "loss" in log]
eval_loss = [log["eval_loss"] for log in logs if "eval_loss" in log]

plt.figure(figsize=(10, 4))
plt.plot(train_loss, label="Train Loss")
plt.plot(eval_loss, label="Validation Loss")
plt.title("EÄŸitim ve Validation KayÄ±p GrafiÄŸi")
plt.xlabel("AdÄ±m")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()

# EÄŸitim sonrasÄ±nda modeli kaydet
trainer.save_model("./results")

from transformers import TrainingArguments, Trainer

# EÄŸitim ayarlarÄ±nÄ± gÃ¼ncelle
training_args = TrainingArguments(
    output_dir="./results",
    logging_strategy="epoch",
    save_strategy="epoch",  # Modeli her epoch sonunda kaydet
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none",
    save_total_limit=2,  # En fazla 2 model kaydÄ±nÄ± tut
)

# Trainer'Ä± tekrar oluÅŸtur
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# EÄŸitimi baÅŸlat
trainer.train()

# Modeli kaydet
trainer.save_model("./results")

# Modeli yÃ¼kle
model = BertForSequenceClassification.from_pretrained("./results", num_labels=3)

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Model ve tokenizer'Ä± yÃ¼kle
model_path = "./results"  # EÄŸitilen modelin kaydedildiÄŸi dizin
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained(model_path)

# SÄ±nÄ±f isimleri
class_names = ["Hate Speech", "Offensive", "Neither"]

def classify_text(text):
    """
    Verilen metni sÄ±nÄ±flandÄ±rÄ±r ve detaylÄ± sonuÃ§ dÃ¶ndÃ¼rÃ¼r.

    Args:
        text (str): Analiz edilecek metin

    Returns:
        dict: {
            'prediction': str (tahmin edilen sÄ±nÄ±f),
            'confidence': float (tahmin gÃ¼ven skoru),
            'probabilities': dict (tÃ¼m sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±),
            'raw_output': list (modelin ham Ã§Ä±ktÄ±larÄ±)
        }
    """
    # Tokenization
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    )

    # Model ile tahmin yap
    with torch.no_grad():
        outputs = model(**inputs)

    # OlasÄ±lÄ±klarÄ± hesapla
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    confidence, predicted_idx = torch.max(probs, dim=-1)

    # SonuÃ§larÄ± formatla
    result = {
        'prediction': class_names[predicted_idx],
        'confidence': confidence.item(),
        'probabilities': {
            class_names[i]: round(probs[0][i].item(), 4) for i in range(len(class_names))
        },
        'raw_output': outputs.logits.tolist()[0]
    }

    return result

# KullanÄ±m Ã¶rnekleri
sample_texts = [
    "I love this place! The staff is amazing!",
    "You're so stupid, I can't believe you did that",
    "All people from that country should be killed"
]

for text in sample_texts:
    print(f"\nMetin: '{text}'")
    result = classify_text(text)
    print(f"Tahmin: {result['prediction']} (%{result['confidence']*100:.2f})")
    print("Detaylar:")
    for cls, prob in result['probabilities'].items():
        print(f"  {cls}: {prob*100:.2f}%")
    print("---")



import pandas as pd

# DosyayÄ± oku
df = pd.read_csv("labeled_data.csv")

# Sadece tweet ve class sÃ¼tunlarÄ±nÄ± al
df = df[['tweet', 'class']]

# Ä°lk 5 satÄ±rÄ± gÃ¶relim
df.head()

import re
import nltk
import pandas as pd
from nltk.corpus import stopwords

# Ä°lk kez Ã§alÄ±ÅŸtÄ±rÄ±yorsan aÅŸaÄŸÄ±dakileri indir
nltk.download('stopwords')

# TÃ¼rkÃ§e ve Ä°ngilizce stopword'leri birleÅŸtir
stop_words = set(stopwords.words('english')).union(set(stopwords.words('turkish')))

def clean_text(text):
    text = text.lower()  # KÃ¼Ã§Ã¼k harfe Ã§evir
    text = re.sub(r"http\S+", "", text)  # Linkleri sil
    text = re.sub(r"@\w+", "", text)  # MentionlarÄ± sil
    text = re.sub(r"#\w+", "", text)  # Hashtagleri sil
    text = re.sub(r"[^\w\s]", "", text)  # Noktalama iÅŸaretlerini sil
    text = re.sub(r"\d+", "", text)  # SayÄ±larÄ± sil
    text = re.sub(r"\s+", " ", text).strip()  # Fazla boÅŸluklarÄ± temizle
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]  # Stopword'leri Ã§Ä±kar
    return " ".join(tokens)

df['CLEAN_TEXT'] = df['tweet'].apply(clean_text)

# Ä°lk 5 satÄ±rÄ± gÃ¶relim
df[['tweet', 'CLEAN_TEXT', 'class']].head()

df.to_csv("cleaned_labeled_data.csv", index=False)

from google.colab import files
files.download("cleaned_labeled_data.csv")

!pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# SÄ±nÄ±f adlarÄ±nÄ± daha anlamlÄ± hale getirelim
class_labels = {
    0: "Hate Speech",
    1: "Offensive Language",
    2: "Neither"
}

# WordCloud Ã§izimi iÃ§in fonksiyon
def plot_wordcloud(data, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(data))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontsize=18)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# Her sÄ±nÄ±f iÃ§in ayrÄ± ayrÄ± word cloud Ã§izelim
for class_id, label in class_labels.items():
    class_data = df[df['class'] == class_id]['CLEAN_TEXT']
    plot_wordcloud(class_data, f"WordCloud - {label}")

from sklearn.model_selection import train_test_split
# Ã–zellik ve etiketleri belirle
X = df['CLEAN_TEXT']  # Girdi olarak temiz metinler
y = df['class']       # Hedef sÄ±nÄ±flar (0, 1, 2)

# EÄŸitim ve test olarak ayÄ±r (Ã¶rnek: %80 eÄŸitim, %20 test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Boyutlara bakalÄ±m
print("EÄŸitim verisi:", len(X_train))
print("Test verisi  :", len(X_test))

# Her sÄ±nÄ±ftaki Ã¶rnek sayÄ±sÄ±
class_counts = df['class'].value_counts().sort_index()
print(class_counts)
# SÄ±nÄ±f adlarÄ±yla etiketleme
class_labels = {
    0: "Hate Speech",
    1: "Offensive Language",
    2: "Neither"
}

# SÄ±nÄ±f adlarÄ±nÄ± etiketleyerek gÃ¶ster
for cls_id, count in class_counts.items():
    print(f"{class_labels[cls_id]}: {count} tweet")
import matplotlib.pyplot as plt

# Grafik Ã§izimi
plt.figure(figsize=(8, 5))
plt.bar(class_counts.index.map(class_labels), class_counts.values, color=["#e74c3c", "#f1c40f", "#2ecc71"])
plt.title("Her SÄ±nÄ±f iÃ§in Veri SayÄ±sÄ±", fontsize=16)
plt.xlabel("SÄ±nÄ±f", fontsize=12)
plt.ylabel("Veri SayÄ±sÄ±", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Ã–rnek olarak tweet dizilimini baÄŸlamsal hale getiriyoruz
context_inputs = []
previous_texts = df['CLEAN_TEXT'].tolist()

for i in range(len(previous_texts)):
    if i == 0:
        context_inputs.append(previous_texts[i])
    else:
        combined = previous_texts[i-1] + " [SEP] " + previous_texts[i]
        context_inputs.append(combined)

# Yeni sÃ¼tun ekle
df['CONTEXT_INPUT'] = context_inputs

# Ä°lk 5 Ã¶rneÄŸi gÃ¶relim
df[['CONTEXT_INPUT', 'class']].head()

# context_input iÃ§eren veri setini kaydet
df.to_csv("context_aware_labeled_data.csv", index=False)

from google.colab import files
files.download("context_aware_labeled_data.csv")

!pip install --upgrade transformers

# âœ… Gerekli Kurulumlar
!pip install --upgrade transformers datasets --quiet

# âœ… GPU kontrolÃ¼
import torch
print("GPU kullanÄ±labilir mi?", torch.cuda.is_available())

# âœ… Gerekli kÃ¼tÃ¼phaneler
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset

# âœ… 1. Veri YÃ¼kleme
df = pd.read_csv("context_aware_labeled_data.csv")

# âœ… 2. Ã–zellik ve etiket
X = df['CONTEXT_INPUT'].tolist()
y = df['class'].tolist()

# âœ… 3. Train/Test split (%80 - %20)
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# âœ… 4. Train/Validation split (%90 - %10)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, stratify=y_train_full, random_state=42)

# âœ… 5. Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# âœ… 6. Dataset sÄ±nÄ±fÄ±
class TweetDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

# âœ… 7. Datasetâ€™leri oluÅŸtur
train_dataset = TweetDataset(X_train, y_train)
val_dataset   = TweetDataset(X_val, y_val)
test_dataset  = TweetDataset(X_test, y_test)

# âœ… 8. Modeli yÃ¼kle
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

# âœ… 9. EÄŸitim ayarlarÄ±
training_args = TrainingArguments(
    output_dir="./results",
    logging_strategy="epoch",  # `evaluation_strategy` yerine `logging_strategy` kullanabilirsiniz
    save_strategy="no",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none"
)


# âœ… 10. Trainer nesnesi
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# âœ… 11. EÄŸitimi baÅŸlat
trainer.train()

# âœ… 12. Train seti Ã¼zerinde tahmin yap
train_preds_output = trainer.predict(train_dataset)
train_preds = torch.argmax(torch.tensor(train_preds_output.predictions), axis=1)
train_labels = torch.tensor(train_preds_output.label_ids)

# âœ… 13. Test seti Ã¼zerinde tahmin yap
test_preds_output = trainer.predict(test_dataset)
test_preds = torch.argmax(torch.tensor(test_preds_output.predictions), axis=1)
test_labels = torch.tensor(test_preds_output.label_ids)

class_names = ["Hate Speech", "Offensive", "Neither"]

# âœ… 14. SÄ±nÄ±flandÄ±rma Raporu (Classification Report)
print("\nğŸ“Š TRAIN SÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(train_labels, train_preds, target_names=class_names))

print("\nğŸ“Š TEST SÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(test_labels, test_preds, target_names=class_names))

# âœ… 15. Confusion Matrix (Train ve Test Setleri)
# Train CM
cm_train = confusion_matrix(train_labels, train_preds)
disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=class_names)
fig, ax = plt.subplots(figsize=(6, 6))
disp_train.plot(cmap="Greens", ax=ax, values_format='d')
plt.title("Confusion Matrix â€“ TRAIN Seti")
plt.show()

# Test CM
cm_test = confusion_matrix(test_labels, test_preds)
disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=class_names)
fig, ax = plt.subplots(figsize=(6, 6))
disp_test.plot(cmap="Blues", ax=ax, values_format='d')
plt.title("Confusion Matrix â€“ TEST Seti")
plt.show()

# âœ… 16. Loss Grafiklerini Ã‡iz (Train ve Validation)
logs = trainer.state.log_history

train_loss = [log["loss"] for log in logs if "loss" in log]
eval_loss = [log["eval_loss"] for log in logs if "eval_loss" in log]

plt.figure(figsize=(10, 4))
plt.plot(train_loss, label="Train Loss")
plt.plot(eval_loss, label="Validation Loss")
plt.title("EÄŸitim ve Validation KayÄ±p GrafiÄŸi")
plt.xlabel("AdÄ±m")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()

# EÄŸitim sonrasÄ±nda modeli kaydet
trainer.save_model("./results")

from transformers import TrainingArguments, Trainer

# EÄŸitim ayarlarÄ±nÄ± gÃ¼ncelle
training_args = TrainingArguments(
    output_dir="./results",
    logging_strategy="epoch",
    save_strategy="epoch",  # Modeli her epoch sonunda kaydet
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none",
    save_total_limit=2,  # En fazla 2 model kaydÄ±nÄ± tut
)

# Trainer'Ä± tekrar oluÅŸtur
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# EÄŸitimi baÅŸlat
trainer.train()

# Modeli kaydet
trainer.save_model("./results")

# Modeli yÃ¼kle
model = BertForSequenceClassification.from_pretrained("./results", num_labels=3)

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Model ve tokenizer'Ä± yÃ¼kle
model_path = "./results"  # EÄŸitilen modelin kaydedildiÄŸi dizin
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained(model_path)

# SÄ±nÄ±f isimleri
class_names = ["Hate Speech", "Offensive", "Neither"]

def classify_text(text):
    """
    Verilen metni sÄ±nÄ±flandÄ±rÄ±r ve detaylÄ± sonuÃ§ dÃ¶ndÃ¼rÃ¼r.

    Args:
        text (str): Analiz edilecek metin

    Returns:
        dict: {
            'prediction': str (tahmin edilen sÄ±nÄ±f),
            'confidence': float (tahmin gÃ¼ven skoru),
            'probabilities': dict (tÃ¼m sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±),
            'raw_output': list (modelin ham Ã§Ä±ktÄ±larÄ±)
        }
    """
    # Tokenization
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    )

    # Model ile tahmin yap
    with torch.no_grad():
        outputs = model(**inputs)

    # OlasÄ±lÄ±klarÄ± hesapla
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    confidence, predicted_idx = torch.max(probs, dim=-1)

    # SonuÃ§larÄ± formatla
    result = {
        'prediction': class_names[predicted_idx],
        'confidence': confidence.item(),
        'probabilities': {
            class_names[i]: round(probs[0][i].item(), 4) for i in range(len(class_names))
        },
        'raw_output': outputs.logits.tolist()[0]
    }

    return result

# KullanÄ±m Ã¶rnekleri
sample_texts = [
    "I love this place! The staff is amazing!",
    "You're so stupid, I can't believe you did that",
    "All people from that country should be killed"
]

for text in sample_texts:
    print(f"\nMetin: '{text}'")
    result = classify_text(text)
    print(f"Tahmin: {result['prediction']} (%{result['confidence']*100:.2f})")
    print("Detaylar:")
    for cls, prob in result['probabilities'].items():
        print(f"  {cls}: {prob*100:.2f}%")
    print("---")

import torch
print(torch.__version__)
print("CUDA mevcut mu?", torch.cuda.is_available())

!pip install -U transformers

import os
os.kill(os.getpid(), 9)



import pandas as pd

# DosyayÄ± oku
df = pd.read_csv("labeled_data.csv")

# Sadece tweet ve class sÃ¼tunlarÄ±nÄ± al
df = df[['tweet', 'class']]

# Ä°lk 5 satÄ±rÄ± gÃ¶relim
df.head()

import re
import nltk
import pandas as pd
from nltk.corpus import stopwords

# Ä°lk kez Ã§alÄ±ÅŸtÄ±rÄ±yorsan aÅŸaÄŸÄ±dakileri indir
nltk.download('stopwords')

# TÃ¼rkÃ§e ve Ä°ngilizce stopword'leri birleÅŸtir
stop_words = set(stopwords.words('english')).union(set(stopwords.words('turkish')))

def clean_text(text):
    text = text.lower()  # KÃ¼Ã§Ã¼k harfe Ã§evir
    text = re.sub(r"http\S+", "", text)  # Linkleri sil
    text = re.sub(r"@\w+", "", text)  # MentionlarÄ± sil
    text = re.sub(r"#\w+", "", text)  # Hashtagleri sil
    text = re.sub(r"[^\w\s]", "", text)  # Noktalama iÅŸaretlerini sil
    text = re.sub(r"\d+", "", text)  # SayÄ±larÄ± sil
    text = re.sub(r"\s+", " ", text).strip()  # Fazla boÅŸluklarÄ± temizle
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]  # Stopword'leri Ã§Ä±kar
    return " ".join(tokens)

df['CLEAN_TEXT'] = df['tweet'].apply(clean_text)

# Ä°lk 5 satÄ±rÄ± gÃ¶relim
df[['tweet', 'CLEAN_TEXT', 'class']].head()

df.to_csv("cleaned_labeled_data.csv", index=False)

from google.colab import files
files.download("cleaned_labeled_data.csv")

!pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# SÄ±nÄ±f adlarÄ±nÄ± daha anlamlÄ± hale getirelim
class_labels = {
    0: "Hate Speech",
    1: "Offensive Language",
    2: "Neither"
}

# WordCloud Ã§izimi iÃ§in fonksiyon
def plot_wordcloud(data, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(data))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontsize=18)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# Her sÄ±nÄ±f iÃ§in ayrÄ± ayrÄ± word cloud Ã§izelim
for class_id, label in class_labels.items():
    class_data = df[df['class'] == class_id]['CLEAN_TEXT']
    plot_wordcloud(class_data, f"WordCloud - {label}")

from sklearn.model_selection import train_test_split
# Ã–zellik ve etiketleri belirle
X = df['CLEAN_TEXT']  # Girdi olarak temiz metinler
y = df['class']       # Hedef sÄ±nÄ±flar (0, 1, 2)

# EÄŸitim ve test olarak ayÄ±r (Ã¶rnek: %80 eÄŸitim, %20 test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Boyutlara bakalÄ±m
print("EÄŸitim verisi:", len(X_train))
print("Test verisi  :", len(X_test))

# Her sÄ±nÄ±ftaki Ã¶rnek sayÄ±sÄ±
class_counts = df['class'].value_counts().sort_index()
print(class_counts)
# SÄ±nÄ±f adlarÄ±yla etiketleme
class_labels = {
    0: "Hate Speech",
    1: "Offensive Language",
    2: "Neither"
}

# SÄ±nÄ±f adlarÄ±nÄ± etiketleyerek gÃ¶ster
for cls_id, count in class_counts.items():
    print(f"{class_labels[cls_id]}: {count} tweet")
import matplotlib.pyplot as plt

# Grafik Ã§izimi
plt.figure(figsize=(8, 5))
plt.bar(class_counts.index.map(class_labels), class_counts.values, color=["#e74c3c", "#f1c40f", "#2ecc71"])
plt.title("Her SÄ±nÄ±f iÃ§in Veri SayÄ±sÄ±", fontsize=16)
plt.xlabel("SÄ±nÄ±f", fontsize=12)
plt.ylabel("Veri SayÄ±sÄ±", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Ã–rnek olarak tweet dizilimini baÄŸlamsal hale getiriyoruz
context_inputs = []
previous_texts = df['CLEAN_TEXT'].tolist()

for i in range(len(previous_texts)):
    if i == 0:
        context_inputs.append(previous_texts[i])
    else:
        combined = previous_texts[i-1] + " [SEP] " + previous_texts[i]
        context_inputs.append(combined)

# Yeni sÃ¼tun ekle
df['CONTEXT_INPUT'] = context_inputs

# Ä°lk 5 Ã¶rneÄŸi gÃ¶relim
df[['CONTEXT_INPUT', 'class']].head()

# context_input iÃ§eren veri setini kaydet
df.to_csv("context_aware_labeled_data.csv", index=False)

from google.colab import files
files.download("context_aware_labeled_data.csv")

!pip install --upgrade transformers

# âœ… Gerekli Kurulumlar
!pip install --upgrade transformers datasets --quiet

# âœ… GPU kontrolÃ¼
import torch
print("GPU kullanÄ±labilir mi?", torch.cuda.is_available())

# âœ… Gerekli kÃ¼tÃ¼phaneler
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset

# âœ… 1. Veri YÃ¼kleme
df = pd.read_csv("context_aware_labeled_data.csv")

# âœ… 2. Ã–zellik ve etiket
X = df['CONTEXT_INPUT'].tolist()
y = df['class'].tolist()

# âœ… 3. Train/Test split (%80 - %20)
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# âœ… 4. Train/Validation split (%90 - %10)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, stratify=y_train_full, random_state=42)

# âœ… 5. Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# âœ… 6. Dataset sÄ±nÄ±fÄ±
class TweetDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

# âœ… 7. Datasetâ€™leri oluÅŸtur
train_dataset = TweetDataset(X_train, y_train)
val_dataset   = TweetDataset(X_val, y_val)
test_dataset  = TweetDataset(X_test, y_test)

# âœ… 8. Modeli yÃ¼kle
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

# âœ… 9. EÄŸitim ayarlarÄ±
training_args = TrainingArguments(
    output_dir="./results",
    logging_strategy="epoch",  # `evaluation_strategy` yerine `logging_strategy` kullanabilirsiniz
    save_strategy="no",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none"
)


# âœ… 10. Trainer nesnesi
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# âœ… 11. EÄŸitimi baÅŸlat
trainer.train()

# âœ… 12. Train seti Ã¼zerinde tahmin yap
train_preds_output = trainer.predict(train_dataset)
train_preds = torch.argmax(torch.tensor(train_preds_output.predictions), axis=1)
train_labels = torch.tensor(train_preds_output.label_ids)

# âœ… 13. Test seti Ã¼zerinde tahmin yap
test_preds_output = trainer.predict(test_dataset)
test_preds = torch.argmax(torch.tensor(test_preds_output.predictions), axis=1)
test_labels = torch.tensor(test_preds_output.label_ids)

class_names = ["Hate Speech", "Offensive", "Neither"]

# âœ… 14. SÄ±nÄ±flandÄ±rma Raporu (Classification Report)
print("\nğŸ“Š TRAIN SÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(train_labels, train_preds, target_names=class_names))

print("\nğŸ“Š TEST SÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(test_labels, test_preds, target_names=class_names))

# âœ… 15. Confusion Matrix (Train ve Test Setleri)
# Train CM
cm_train = confusion_matrix(train_labels, train_preds)
disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=class_names)
fig, ax = plt.subplots(figsize=(6, 6))
disp_train.plot(cmap="Greens", ax=ax, values_format='d')
plt.title("Confusion Matrix â€“ TRAIN Seti")
plt.show()

# Test CM
cm_test = confusion_matrix(test_labels, test_preds)
disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=class_names)
fig, ax = plt.subplots(figsize=(6, 6))
disp_test.plot(cmap="Blues", ax=ax, values_format='d')
plt.title("Confusion Matrix â€“ TEST Seti")
plt.show()

# âœ… 16. Loss Grafiklerini Ã‡iz (Train ve Validation)
logs = trainer.state.log_history

train_loss = [log["loss"] for log in logs if "loss" in log]
eval_loss = [log["eval_loss"] for log in logs if "eval_loss" in log]

plt.figure(figsize=(10, 4))
plt.plot(train_loss, label="Train Loss")
plt.plot(eval_loss, label="Validation Loss")
plt.title("EÄŸitim ve Validation KayÄ±p GrafiÄŸi")
plt.xlabel("AdÄ±m")
plt.ylabel("Loss")
plt.legend()
plt.grid()
plt.show()

# EÄŸitim sonrasÄ±nda modeli kaydet
trainer.save_model("./results")

from transformers import TrainingArguments, Trainer

# EÄŸitim ayarlarÄ±nÄ± gÃ¼ncelle
training_args = TrainingArguments(
    output_dir="./results",
    logging_strategy="epoch",
    save_strategy="epoch",  # Modeli her epoch sonunda kaydet
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none",
    save_total_limit=2,  # En fazla 2 model kaydÄ±nÄ± tut
)

# Trainer'Ä± tekrar oluÅŸtur
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# EÄŸitimi baÅŸlat
trainer.train()

# Modeli kaydet
trainer.save_model("./results")

# Modeli yÃ¼kle
model = BertForSequenceClassification.from_pretrained("./results", num_labels=3)

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Model ve tokenizer'Ä± yÃ¼kle
model_path = "./results"  # EÄŸitilen modelin kaydedildiÄŸi dizin
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained(model_path)

# SÄ±nÄ±f isimleri
class_names = ["Hate Speech", "Offensive", "Neither"]

def classify_text(text):
    """
    Verilen metni sÄ±nÄ±flandÄ±rÄ±r ve detaylÄ± sonuÃ§ dÃ¶ndÃ¼rÃ¼r.

    Args:
        text (str): Analiz edilecek metin

    Returns:
        dict: {
            'prediction': str (tahmin edilen sÄ±nÄ±f),
            'confidence': float (tahmin gÃ¼ven skoru),
            'probabilities': dict (tÃ¼m sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±),
            'raw_output': list (modelin ham Ã§Ä±ktÄ±larÄ±)
        }
    """
    # Tokenization
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    )

    # Model ile tahmin yap
    with torch.no_grad():
        outputs = model(**inputs)

    # OlasÄ±lÄ±klarÄ± hesapla
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    confidence, predicted_idx = torch.max(probs, dim=-1)

    # SonuÃ§larÄ± formatla
    result = {
        'prediction': class_names[predicted_idx],
        'confidence': confidence.item(),
        'probabilities': {
            class_names[i]: round(probs[0][i].item(), 4) for i in range(len(class_names))
        },
        'raw_output': outputs.logits.tolist()[0]
    }

    return result

# KullanÄ±m Ã¶rnekleri
sample_texts = [
    "I love this place! The staff is amazing!",
    "You're so stupid, I can't believe you did that",
    "All people from that country should be killed"
]

for text in sample_texts:
    print(f"\nMetin: '{text}'")
    result = classify_text(text)
    print(f"Tahmin: {result['prediction']} (%{result['confidence']*100:.2f})")
    print("Detaylar:")
    for cls, prob in result['probabilities'].items():
        print(f"  {cls}: {prob*100:.2f}%")
    print("---")



import torch
print(torch.__version__)
print("CUDA mevcut mu?", torch.cuda.is_available())

!pip install -U transformers

import os
os.kill(os.getpid(), 9)

import os
os.environ["WANDB_DISABLED"] = "true"  # wandb tamamen devre dÄ±ÅŸÄ±
import torch
from torch import cuda
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset

# GPU kontrolÃ¼ ve ayarlarÄ±
device = 'cuda' if cuda.is_available() else 'cpu'
print(f"âš¡ KullanÄ±lan cihaz: {device}")
print(f"ğŸ” GPU Detay: {torch.cuda.get_device_name(0) if cuda.is_available() else 'CPU kullanÄ±lÄ±yor'}")

# Veri yÃ¼kleme
df = pd.read_csv("context_aware_labeled_data.csv")
X = df['CONTEXT_INPUT'].tolist()  # Ä°ngilizce metinler
y = df['class'].tolist()
class_names = ["Hate Speech", "Offensive Language", "Neither"]

# Train/Test split (%80-%20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Ã–zel Dataset sÄ±nÄ±fÄ± (GPU'ya otomatik yÃ¼kleme)
class EnglishTweetDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.encodings = tokenizer(
            texts,
            truncation=True,
            padding='max_length',
            max_length=max_length,
            return_tensors="pt"
        )
        self.labels = torch.tensor(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.encodings['input_ids'][idx],         # .to(device) YOK
            'attention_mask': self.encodings['attention_mask'][idx],
            'labels': self.labels[idx]
        }

# RoBERTa tokenizer ve model
roberta_tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
roberta_model = RobertaForSequenceClassification.from_pretrained(
    "roberta-base",
    num_labels=3
).to(device)

# EÄŸitim ayarlarÄ± (Ä°ngilizce iÃ§in optimize)
# EÄŸitim ayarlarÄ± (eski versiyon transformers iÃ§in sadeleÅŸtirilmiÅŸ)
training_args = TrainingArguments(
    output_dir="./results_roberta_en",
    per_device_train_batch_size=8,
    num_train_epochs=4,
    logging_dir="./logs_roberta_en"
)


# Trainer
trainer = Trainer(
    model=roberta_model,
    args=training_args,
    train_dataset=EnglishTweetDataset(X_train, y_train, roberta_tokenizer),
    eval_dataset=EnglishTweetDataset(X_test, y_test, roberta_tokenizer)
)

# EÄŸitimi baÅŸlat
print("ğŸ”¥ RoBERTa modeli eÄŸitiliyor...")
trainer.train()

# â­â­â­ BU SATIRLARI EKLEYÄ°N â­â­â­
# Modeli ve tokenizer'Ä± kaydet
trainer.save_model("results_roberta_en")  # "./" olmadan
tokenizer.save_pretrained("results_roberta_en")

# Kaydedilen dosyalarÄ± kontrol et
print("\nğŸ’¾ Kaydedilen model dosyalarÄ±:")
!ls -l results_roberta_en

# GPU belleÄŸini temizle
torch.cuda.empty_cache()
print("âœ… EÄŸitim tamamlandÄ± ve model kaydedildi!")

# Test setinde deÄŸerlendirme
print("ğŸ§ª Model test ediliyor...")
test_results = trainer.predict(EnglishTweetDataset(X_test, y_test, roberta_tokenizer))
y_pred = torch.argmax(torch.tensor(test_results.predictions), dim=1).cpu().numpy()

# DetaylÄ± metrikler
print("\nğŸ“Š RoBERTa Classification Report (Test Seti):")
print(classification_report(y_test, y_pred, target_names=class_names, digits=4))

# Confusion Matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap="Blues", values_format='d')
plt.title("RoBERTa - Confusion Matrix (Ä°ngilizce Veri)\n", fontsize=14, pad=20)
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("roberta_confusion_matrix_en.png", dpi=300)
plt.show()

import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification

# Model ve tokenizer yÃ¼kleniyor
model_path = "results_roberta_en"  # Modelin kaydedildiÄŸi dizin
tokenizer = RobertaTokenizer.from_pretrained(model_path)
model = RobertaForSequenceClassification.from_pretrained(model_path)

# GPU'ya gÃ¶nder (varsa)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# SÄ±nÄ±f isimleri
class_names = ["Hate Speech", "Offensive Language", "Neither"]

def predict_text(text, confidence_threshold=0.7):
    """
    RoBERTa modeli ile metin sÄ±nÄ±flandÄ±rma

    Args:
        text (str): Analiz edilecek metin
        confidence_threshold (float): Minimum gÃ¼ven skoru (0-1 arasÄ±)

    Returns:
        dict: {
            'prediction': str (tahmin edilen sÄ±nÄ±f),
            'confidence': float (gÃ¼ven skoru),
            'probabilities': dict (tÃ¼m sÄ±nÄ±f olasÄ±lÄ±klarÄ±),
            'is_confident': bool (eÅŸik deÄŸerini aÅŸÄ±yor mu)
        }
    """
    # Tokenizasyon
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    ).to(device)

    # Tahmin
    with torch.no_grad():
        outputs = model(**inputs)

    # OlasÄ±lÄ±k hesaplama
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    confidence, pred_idx = torch.max(probs, dim=-1)

    # SonuÃ§ formatlama
    return {
        'prediction': class_names[pred_idx],
        'confidence': round(confidence.item(), 4),
        'probabilities': {
            class_names[i]: round(prob.item(), 4) for i, prob in enumerate(probs[0])
        },
        'is_confident': confidence.item() >= confidence_threshold
    }

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    test_texts = [
        "You're completely useless",
        "You're so stupid!",
        "This place is really nice and calm, and everyone is very friendly."
    ]

    for text in test_texts:
        result = predict_text(text)
        print(f"\nğŸ“ Metin: {text[:50]}{'...' if len(text)>50 else ''}")
        print(f"ğŸ”® Tahmin: {result['prediction']} (GÃ¼ven: {result['confidence']*100:.1f}%)")
        print("ğŸ“Š OlasÄ±lÄ±klar:")
        for cls, prob in result['probabilities'].items():
            print(f"   - {cls}: {prob*100:.1f}%")
        if not result['is_confident']:
            print("âš ï¸ DÃ¼ÅŸÃ¼k gÃ¼ven skoru - SonuÃ§ belirsiz olabilir!")
        print("â”"*50)



import torch
print(torch.__version__)
print("CUDA mevcut mu?", torch.cuda.is_available())

!pip install -U transformers

import os
os.kill(os.getpid(), 9)

import os
os.environ["WANDB_DISABLED"] = "true"  # wandb tamamen devre dÄ±ÅŸÄ±
import torch
from torch import cuda
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from torch.utils.data import Dataset

# GPU kontrolÃ¼ ve ayarlarÄ±
device = 'cuda' if cuda.is_available() else 'cpu'
print(f"âš¡ KullanÄ±lan cihaz: {device}")
print(f"ğŸ” GPU Detay: {torch.cuda.get_device_name(0) if cuda.is_available() else 'CPU kullanÄ±lÄ±yor'}")

# Veri yÃ¼kleme
df = pd.read_csv("context_aware_labeled_data.csv")
X = df['CONTEXT_INPUT'].tolist()  # Ä°ngilizce metinler
y = df['class'].tolist()
class_names = ["Hate Speech", "Offensive Language", "Neither"]

# Train/Test split (%80-%20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Ã–zel Dataset sÄ±nÄ±fÄ± (GPU'ya otomatik yÃ¼kleme)
class EnglishTweetDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.encodings = tokenizer(
            texts,
            truncation=True,
            padding='max_length',
            max_length=max_length,
            return_tensors="pt"
        )
        self.labels = torch.tensor(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'input_ids': self.encodings['input_ids'][idx],         # .to(device) YOK
            'attention_mask': self.encodings['attention_mask'][idx],
            'labels': self.labels[idx]
        }

# RoBERTa tokenizer ve model
roberta_tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
roberta_model = RobertaForSequenceClassification.from_pretrained(
    "roberta-base",
    num_labels=3
).to(device)

# EÄŸitim ayarlarÄ± (Ä°ngilizce iÃ§in optimize)
# EÄŸitim ayarlarÄ± (eski versiyon transformers iÃ§in sadeleÅŸtirilmiÅŸ)
training_args = TrainingArguments(
    output_dir="./results_roberta_en",
    per_device_train_batch_size=8,
    num_train_epochs=4,
    logging_dir="./logs_roberta_en"
)


# Trainer
trainer = Trainer(
    model=roberta_model,
    args=training_args,
    train_dataset=EnglishTweetDataset(X_train, y_train, roberta_tokenizer),
    eval_dataset=EnglishTweetDataset(X_test, y_test, roberta_tokenizer)
)

# EÄŸitimi baÅŸlat
print("ğŸ”¥ RoBERTa modeli eÄŸitiliyor...")
trainer.train()

# â­â­â­ BU SATIRLARI EKLEYÄ°N â­â­â­
# Modeli ve tokenizer'Ä± kaydet
trainer.save_model("results_roberta_en")  # "./" olmadan
tokenizer.save_pretrained("results_roberta_en")

# Kaydedilen dosyalarÄ± kontrol et
print("\nğŸ’¾ Kaydedilen model dosyalarÄ±:")
!ls -l results_roberta_en

# GPU belleÄŸini temizle
torch.cuda.empty_cache()
print("âœ… EÄŸitim tamamlandÄ± ve model kaydedildi!")

# Test setinde deÄŸerlendirme
print("ğŸ§ª Model test ediliyor...")
test_results = trainer.predict(EnglishTweetDataset(X_test, y_test, roberta_tokenizer))
y_pred = torch.argmax(torch.tensor(test_results.predictions), dim=1).cpu().numpy()

# DetaylÄ± metrikler
print("\nğŸ“Š RoBERTa Classification Report (Test Seti):")
print(classification_report(y_test, y_pred, target_names=class_names, digits=4))

# Confusion Matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap="Blues", values_format='d')
plt.title("RoBERTa - Confusion Matrix (Ä°ngilizce Veri)\n", fontsize=14, pad=20)
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("roberta_confusion_matrix_en.png", dpi=300)
plt.show()

import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification

# Model ve tokenizer yÃ¼kleniyor
model_path = "results_roberta_en"  # Modelin kaydedildiÄŸi dizin
tokenizer = RobertaTokenizer.from_pretrained(model_path)
model = RobertaForSequenceClassification.from_pretrained(model_path)

# GPU'ya gÃ¶nder (varsa)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# SÄ±nÄ±f isimleri
class_names = ["Hate Speech", "Offensive Language", "Neither"]

def predict_text(text, confidence_threshold=0.7):
    """
    RoBERTa modeli ile metin sÄ±nÄ±flandÄ±rma

    Args:
        text (str): Analiz edilecek metin
        confidence_threshold (float): Minimum gÃ¼ven skoru (0-1 arasÄ±)

    Returns:
        dict: {
            'prediction': str (tahmin edilen sÄ±nÄ±f),
            'confidence': float (gÃ¼ven skoru),
            'probabilities': dict (tÃ¼m sÄ±nÄ±f olasÄ±lÄ±klarÄ±),
            'is_confident': bool (eÅŸik deÄŸerini aÅŸÄ±yor mu)
        }
    """
    # Tokenizasyon
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    ).to(device)

    # Tahmin
    with torch.no_grad():
        outputs = model(**inputs)

    # OlasÄ±lÄ±k hesaplama
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    confidence, pred_idx = torch.max(probs, dim=-1)

    # SonuÃ§ formatlama
    return {
        'prediction': class_names[pred_idx],
        'confidence': round(confidence.item(), 4),
        'probabilities': {
            class_names[i]: round(prob.item(), 4) for i, prob in enumerate(probs[0])
        },
        'is_confident': confidence.item() >= confidence_threshold
    }

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    test_texts = [
        "You're completely useless",
        "You're so stupid!",
        "This place is really nice and calm, and everyone is very friendly."
    ]

    for text in test_texts:
        result = predict_text(text)
        print(f"\nğŸ“ Metin: {text[:50]}{'...' if len(text)>50 else ''}")
        print(f"ğŸ”® Tahmin: {result['prediction']} (GÃ¼ven: {result['confidence']*100:.1f}%)")
        print("ğŸ“Š OlasÄ±lÄ±klar:")
        for cls, prob in result['probabilities'].items():
            print(f"   - {cls}: {prob*100:.1f}%")
        if not result['is_confident']:
            print("âš ï¸ DÃ¼ÅŸÃ¼k gÃ¼ven skoru - SonuÃ§ belirsiz olabilir!")
        print("â”"*50)

!pip install transformers datasets --upgrade --quiet
!pip install -U transformers --quiet
!pip install matplotlib seaborn --quiet

# GPU kontrolÃ¼
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"KullanÄ±lan cihaz: {device}")

# Gerekli kÃ¼tÃ¼phaneler
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    confusion_matrix,
    precision_score,
    recall_score,
    f1_score
)
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import (
    XLMRobertaTokenizer,
    XLMRobertaForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import Dataset
import os

# Veri yÃ¼kleme
try:
    df = pd.read_csv("context_aware_labeled_data.csv")
    print("Veri baÅŸarÄ±yla yÃ¼klendi. Ã–rnek veriler:")
    print(df.head())
except FileNotFoundError:
    print("HATA: 'context_aware_labeled_data.csv' dosyasÄ± bulunamadÄ±.")
    exit()

# Veri hazÄ±rlÄ±ÄŸÄ±
X = df['CONTEXT_INPUT']
y = df['class']

# SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± kontrol et
print("\nSÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:")
print(y.value_counts())

# EÄŸitim-test ayrÄ±mÄ±
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    stratify=y,
    test_size=0.2,
    random_state=42
)
train_df = pd.DataFrame({'text': X_train, 'label': y_train})
test_df = pd.DataFrame({'text': X_test, 'label': y_test})

# Model ve tokenizer
model_name = "xlm-roberta-base"
try:
    tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)
    num_labels = len(df['class'].unique())
    print(f"\nModel yÃ¼kleniyor: {model_name} (Etiket sayÄ±sÄ±: {num_labels})")

    model = XLMRobertaForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels
    ).to(device)
except Exception as e:
    print(f"\nHATA: Model yÃ¼klenirken hata oluÅŸtu: {e}")
    exit()

# Tokenizasyon fonksiyonu
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=256
    )

# Dataset oluÅŸturma
try:
    print("\nVeri setleri hazÄ±rlanÄ±yor...")
    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)

    train_dataset = train_dataset.map(tokenize_function, batched=True)
    test_dataset = test_dataset.map(tokenize_function, batched=True)

    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
except Exception as e:
    print(f"\nHATA: Veri seti hazÄ±rlanÄ±rken hata oluÅŸtu: {e}")
    exit()

# EÄŸitim ayarlarÄ± (GÃœNCELLENMÄ°Å VERSÄ°YON)
training_args = TrainingArguments(
    output_dir="./xlmr_results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    eval_strategy="epoch",  # evaluation_strategy yerine eval_strategy
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    disable_tqdm=False,
    no_cuda=not torch.cuda.is_available(),
    report_to="none",
    save_total_limit=2,
)

# Metrik hesaplama
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if isinstance(predictions, tuple):
        predictions = predictions[0]
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    precision = precision_score(labels, predictions, average='weighted')
    recall = recall_score(labels, predictions, average='weighted')
    f1 = f1_score(labels, predictions, average='weighted')

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)

# EÄŸitim
print("\nEÄŸitim baÅŸlÄ±yor...")
try:
    train_result = trainer.train()
    print("\nEÄŸitim tamamlandÄ±!")

    metrics = train_result.metrics
    print(f"\nEÄŸitim Metrikleri:")
    print(f"- Toplam EÄŸitim SÃ¼resi: {metrics['train_runtime']:.2f} saniye")
    print(f"- Ã–rnek baÅŸÄ±na sÃ¼re: {metrics['train_samples_per_second']:.2f} Ã¶rnek/saniye")
    print(f"- KayÄ±p: {metrics['train_loss']:.4f}")

except Exception as e:
    print(f"\nHATA: EÄŸitim sÄ±rasÄ±nda hata oluÅŸtu: {e}")
    if "CUDA out of memory" in str(e):
        print("Bellek hatasÄ±! Batch boyutunu kÃ¼Ã§Ã¼ltmeyi deneyin.")
    exit()

# DeÄŸerlendirme
print("\nTest seti Ã¼zerinde deÄŸerlendirme yapÄ±lÄ±yor...")
try:
    eval_results = trainer.evaluate()
    print("\nDeÄŸerlendirme SonuÃ§larÄ±:")
    print(f"- Accuracy: {eval_results['eval_accuracy']:.4f}")
    print(f"- Precision: {eval_results['eval_precision']:.4f}")
    print(f"- Recall: {eval_results['eval_recall']:.4f}")
    print(f"- F1-Score: {eval_results['eval_f1']:.4f}")

except Exception as e:
    print(f"\nHATA: DeÄŸerlendirme sÄ±rasÄ±nda hata oluÅŸtu: {e}")

# Tahminler ve detaylÄ± analiz
print("\nTahminler yapÄ±lÄ±yor ve detaylÄ± analiz...")
try:
    predictions = trainer.predict(test_dataset)
    preds = np.argmax(predictions.predictions, axis=1)  # DÃ¼zeltme: predictions.predictions

    # SÄ±nÄ±f isimleri
    target_names = ["Hate Speech", "Offensive Language", "Neither"]

    # KapsamlÄ± sÄ±nÄ±flandÄ±rma raporu
    print("\nğŸ“Š DetaylÄ± SÄ±nÄ±flandÄ±rma Raporu:")
    print(classification_report(
        y_test,
        preds,
        target_names=target_names,
        digits=4
    ))

    # Confusion Matrix
    cm = confusion_matrix(y_test, preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=target_names,
        yticklabels=target_names
    )
    plt.title('Confusion Matrix', pad=20)
    plt.xlabel('Tahmin Edilen SÄ±nÄ±f')
    plt.ylabel('GerÃ§ek SÄ±nÄ±f')
    plt.xticks(rotation=45)
    plt.yticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # SÄ±nÄ±f bazÄ±nda metrikler
    print("\nğŸ” SÄ±nÄ±f BazÄ±nda Performans:")
    for i, class_name in enumerate(target_names):
        print(f"\n{class_name}:")
        print(f"- Precision: {precision_score(y_test, preds, average=None)[i]:.4f}")
        print(f"- Recall: {recall_score(y_test, preds, average=None)[i]:.4f}")
        print(f"- F1-Score: {f1_score(y_test, preds, average=None)[i]:.4f}")

    # Ã–rnek tahminler
    print("\nğŸ¯ Ã–rnek Tahminler:")
    sample_df = test_df.copy()
    sample_df['prediction'] = preds
    print(sample_df.sample(5, random_state=42))

except Exception as e:
    print(f"\nHATA: Tahminler sÄ±rasÄ±nda hata oluÅŸtu: {e}")

# Model kaydetme
print("\nModel kaydediliyor...")
try:
    model.save_pretrained("./xlmr_final_model")
    tokenizer.save_pretrained("./xlmr_final_model")
    print("Model baÅŸarÄ±yla kaydedildi: './xlmr_final_model' klasÃ¶rÃ¼ne")
except Exception as e:
    print(f"\nHATA: Model kaydedilirken hata oluÅŸtu: {e}")

# Son bilgiler
print("\nâ„¹ï¸ EÄŸitim Bilgileri:")
print(f"- Toplam EÄŸitim Ã–rnek SayÄ±sÄ±: {len(train_df)}")
print(f"- Toplam Test Ã–rnek SayÄ±sÄ±: {len(test_df)}")
print(f"- KullanÄ±lan Model: {model_name}")
print(f"- EÄŸitim Epoch SayÄ±sÄ±: {training_args.num_train_epochs}")
print(f"- Batch Boyutu (EÄŸitim): {training_args.per_device_train_batch_size}")
print(f"- Batch Boyutu (DeÄŸerlendirme): {training_args.per_device_eval_batch_size}")

import torch
import numpy as np
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification

# Tokenizer ve model yÃ¼kleme (Zaten yÃ¼klediÄŸiniz model kullanÄ±lÄ±yor)
tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")
model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=3).to(device)

# SÄ±nÄ±flarÄ±n isimleri
class_names = ["Hate Speech", "Offensive Language", "Neither"]

# Loss fonksiyonuna aÄŸÄ±rlÄ±k eklemek iÃ§in her sÄ±nÄ±f iÃ§in aÄŸÄ±rlÄ±k belirleyelim
class_weights = torch.tensor([1.0, 1.0, 2.0]).to(device)  # 'Neither' sÄ±nÄ±fÄ±na daha fazla aÄŸÄ±rlÄ±k veriyoruz

# Tokenization ve tahmin fonksiyonu
def predict(texts):
    # Tokenizasyon
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=256).to(device)

    # Model tahminleri
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # Softmax ile olasÄ±lÄ±k hesaplama
    probs = torch.nn.functional.softmax(logits, dim=-1)

    # Her bir metin iÃ§in tahmin ve sÄ±nÄ±f olasÄ±lÄ±klarÄ±nÄ± elde et
    predictions = torch.argmax(probs, dim=-1)

    # Tahminleri sÄ±nÄ±f isimlerine dÃ¶nÃ¼ÅŸtÃ¼r
    predicted_classes = [class_names[prediction] for prediction in predictions]

    return predicted_classes, probs

# Ã–rnek metinler
texts = [
    "God damn it!!",
    "I hate you!",
    "You're the worst!"
]

# Tahmin yapma
predictions, probs = predict(texts)

# SonuÃ§larÄ± yazdÄ±r
for text, prediction, prob in zip(texts, predictions, probs):
    print(f"Text: {text}")
    print(f"Predicted Class: {prediction}")
    print(f"Class Probabilities: {prob}\n")